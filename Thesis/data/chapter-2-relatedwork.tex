\chapter{相关技术介绍}
在自然语言处理研究领域中，基于神经网络的分布式表示（Distributional Representation）通常被称作为词向量（Word Vector）、词嵌入（Word Embedding）或者分布式表示（Distributed Representation）~\upcite{DBLP:conf/nips/MikolovSCCD13}。基于神经网络的词向量表示技术是通过对语言模型的两个组成部分进行建模，即：1）单词的语境，也称为上下文（Context）；2）以及上下文与下一步的目标单词之间的关系（Next-Utterance Prediction）。
神经网络结构和组合方式相对灵活，可以表示更加复杂的上下文语义结构（Complex Context）。
历史上的词向量表示方法主要是基于单词共现矩阵（Co-occurrence Matrix），仅仅是利用了单词的顺序信息。
单词的语义向量是通过将单词共现矩阵进行矩阵分解（Matrix Decomposition），例如奇异值分解算法（Singular Value Decomposition，SVD）、潜在语义分析模型（Latent Semantic Analysis，LSA）或者潜在语义索引（Latent Semantic Indexing，LSI)等分解算法。其中对于SVD分解算法，第一个分解获得的矩阵就是我们需要的单词的词向量矩阵。
N-gram~算法也是使用单词的顺序信息作为上下文信息建模策略，当~$N$~线性增加时，N-gram~模型所需要存储的单词序列组合总数会呈指数爆炸增加，即维数灾难问题（Curse of Dimensionality）\footnote{https://en.wikipedia.org/wiki/Curse\_of\_dimensionality}，它深深限制了~N-gram~模型的实际应用。
假若使用神经网络替代~N-gram~模型来表示单词概率分布，当~$N$~线性增加时神经网络语言模型所需要的参数数量仅以线性速度增长。
这样我们就可以利用神经网络模型可以对更复杂的文本结构进行建模，在从训练后的模型获得的词向量中，将包含更多的语义信息、结构知识。

到目前为止，在语言模型的领域中，研究者提出的神经网络模型主要包括： 传统前向传递神经网络（Feed Forward Neural Network, FFNN）、循环神经网络（Recurrent Neural Network, RNN）三种基本的建模方案。当然基于这些基本组件，我们还可以构造更复杂的模型结构，但是我们无法保证复杂模型一定在效率和精确度上优于简洁的模型结构。复杂的模型可以表征更复杂的本文语义信息，例如句子的依存关系（Dependency）结构或者句子的句法分析树（Syntactic Structure）结构，但是这样的效果不一定能保证模型的泛化性能（Generalization）会很好，因为日常聊天文本的语序是存在较多混乱的数据，很难找到良好定义的句子关系，所以也无法让模型从混乱的文本分布中学习到有效的句子结构，以提升模型的精度。显然是假设越弱模型越好，因为针对复杂的网络文本数据，我们模型训练所需要的要求越弱，模型越容易收敛并获得我们想要的结果。模型假设越强，模型所需要的数据质量越高，实际稳定性就会降低，产生得不偿失的后果。


除此以外，针对在本论文所讨论的语言模型的大规模应用过程中所遇到的大词表问题，历史文献中提出的解决方法主要可以划分为：单词拆分算法（Vocabulary Truncation）、采样估计模型（Sampling-based Approximation）和词表层次分解（Vocabulary Factorisation），我们会在接下来的章节陆续介绍。其中本论文重点是研究基于类别的多元分类模型（class-based hierarchical softmax， cHSM）和基于二叉树的二元分类模型（class-based hierarchical softmax，tHSM），这两种算法我们分别在下一章更详细讨论和介绍。

\section{语言模型}
在这一节中，我们将首先从语言模型提出的实际背景的形式化定义开始，以阐述语言模型的训练目标和实际应用场景~\upcite{DBLP:journals/tnn/ChienK16}。一个好的语言模型应当考虑对文本的两种特征进行建模：语法特征（Syntactics）和语义特征（Semantics）。为了保证句子的语法正确，我们需要考虑的是所生成的单词的前面的上下文（Previous Context），这也就意味着语法特性往往只对局部特征（Local Representation）进行建模。而语义特征的一致性则复杂了许多，它意味着我们需要考虑更大、更完善的上下文信息乃至整个文档语料，来获取全局语义（Global Context）而不仅仅是局部语义信息（Local Context）。


具体地来说，给定一个包含$T$个单词的序列：$w_1,w_2,\cdots,w_T$，这个序列的概率或者对数概率（Log-probability），即描述该序列作为一个合理且合法的句子的流畅性（Fluency）和出现该单词组合的句子的可能性（ Likelihood）。由于直接求解整个序列的概率比较复杂，所以我们将采用马尔可夫链规则（Markov Chain）~\footnote{https://en.wikipedia.org/wiki/Markov\_chain}，分解成一系列的条件概率（Conditional Probability）的联合分布（Joint Distribution）：
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\label{laguage_model}
 \log p(w_1,\cdots, w_T ) = \sum_{t=1}^T \log p(w_t | w_{1:t-1}),
\end{equation}
其中前 $t-1$个单词在本论文中一律记作$w_ {1:t-1}$。进而，条件概率~$p(w_t | w_ {1:t-1})$表示给定其前面上下文$w_ {1:t-1} $作为预测的下一个单词的条件概率分布（Conditional Probability）。最早的传统方法可以由前馈网络来建模这个上下文信息， 同时用 softmax 函数来表示下一个单词的概率分布计算公式~\upcite{DBLP:journals/jmlr/BengioDVJ03}。 在训练的过程中，整个模型以最小化每个单词预测的交叉熵损失（Cross Entropy Loss）为目标：
\begin{equation}\label{equ:losses2}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
  \ell=\sum_{t=1}^{T}\ell_t=\sum_{t=1}^{T}\log p(w_t | w_{1:t-1})
\end{equation}
其中文本或者句子的交叉熵代价函数$\ell$ 的实际意义为：是当编码方案不一定完美时，平均编码长度是多少（最优的编码长度就是1）。这里的编码长度的概念可以理解为：如何用01字节对所有的单词进行最短的编码，保证单词之间两两不互相冲突。交叉熵代价函数是由$\texttt{KL}$散度（Kullback–Leibler Divergence），也叫做相对熵（Relative Entropy），推导而获得的。这里我们需要注意的是，$\mathtt{KL}$散度的计算公式是：
\begin{equation}\label{equ:losses}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
  \mathtt{KL}(p||q)=-\sum_{t=1}^{T}p(x)\log q(x) - (\sum_{t=1}^{T}p(x)\log p(x))
\end{equation}
其中$p(x)$ 指的是文本数据的真实概率分布，$q(x)$ 是由模型从数据中计算得到的预测概率分布。由于$p(x)$ 和$q(x)$ 在公式中的地位不是相等的，所以$\texttt{KL} (p\parallel q)\not\equiv \texttt{KL}(q\parallel p)$。对于给定的文本语料训练数据集，第二项的计算值是常数，由此可知其导数是0，它不会对模型中的参数更新产生任何影响。所以在通常情况下，我们采用交叉熵函数$\ell$作为模型的代价函数而不是相对熵代价函数$\mathtt{KL}$，但其实我们希望模型优化的目标是$\mathtt{KL}$使得模型的分布拟合实际的分布。
\subsection{语言模型的应用}
语言模型的实际用途包括垃圾邮件分类、机器翻译或者自动写诗歌等，当我们对这些纷繁复杂的应用作一定抽象之后，可以将语言模型的主要作用分为以下三种情况：1） 为句子打分（Sentence Scoring）。给定一个未知的单词序列，我们需要计算其作为一个句子的流畅性和出现的可能性；2） 挑选最佳的候选词（Next-utterance Prediction）。给定上下文，选择概率最高的单词。这里所指的上下文的形式是广义的，它可能是：前面的句子给定，选择下一个最佳单词；前面和后面的句子给出，选择中间最适配的单词；后面的句子给定，选择上一个最佳单词（逆序文本建模也是有其独特应用地位的）；3） 训练词向量（Pretrained Word Embedding）。语言模型的输入，在经过大规模数据集训练之后，可以作为词向量输出，从而为其他文本任务提供有效的单词预训练特征。

\subsection{长距离依赖}
不同于前馈神经网络语言模型，循环神经网络语言模型使用隐藏层状态（Hidden State）来记录单词的顺序性信息（Sequence Order），它能够捕捉句子中存在的长距离依赖关系（Long-term Dependency）。这里的长距离依赖主要指语义对应关系（Semantic Correspondence）和选择限制（Selectional Preferences）\upcite{贾玉祥2014汉语语义选择限制知识的自动获取研究}。

一方面，两个单词距离较远但是 在文本中会存在很强的相关性关系。这里的所指的相关性，包括两个单词具有语义对应关系，或者他们需要满足句子的语法结构（Grammatical Structure）。如果采用传统的~N-gram~建模策略，我们必须要建模N-gram（$N>3$）单词概率分布，才能保证模型能学习到这种对应关系。

另一方面，文本中存在的依赖关系将会限制我们选择接下来的单词候选集，称其为选择限制关系。
如图~\ref{fig:lm}~所示，模型在预测过程中，在每一步都需要对词表中所有单词做预测，我们需要将每一步都做选择限制，减少模型的预测分支，这样才能保证模型生成的结果尽可能的语义一致且连贯。每次开放式的对所有单词做预测所带来的后果可能是，那些高频单词会不断出现在预测序列里面，因为训练语聊里面这些单词出现频率就非常高，所以模型将这些高频词预测概率大也是合乎情理的。但问题出现在，一旦我们模型不断预测高频词，我们生成的句子很难传达有效的语义信息。

\begin{figure}[!b]
  \centering
  \includegraphics[width=.99\columnwidth]{./figures/lm.pdf}
  \caption{循环神经网络语言模型图例}
  \label{fig:lm}
\end{figure}

\subsection{循环神经网络}
考虑到上面描述长距离依赖问题，在本实验中，我们将采用的基准模型是循环神经网络语言模型。近年来，基于时间序列建模的循环神经网络已经非常流行。如图~\ref{fig:lm}~所示，由于在当前时间步 $t$的输出$w_{t+1}$正好是下一个时间步的输入$w_{t+1}$，所以将上一步的输出接到下一步的输入。当我们需要做文本生成时，才需要利用这样的自循环链接（Auto-associative Connection）。每个句子都需要用开始词（即，$\langle s\rangle$）和结束词（即$\langle / s\rangle$）标记进行封装。 在预测下一个单词 $\hat w_{t+1}$ 之前，需要用到前一时刻的隐藏状态$h_{t-1}$和当前读入的单词~$w_t$~来计算网络单元的输入$h_t$。
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
  \hat w_{t+1}=\arg\max_{w\in\mathcal{V}} \mathtt{softmax}(Vh_t+d)
\end{equation}
从形式上讲，循环神经网络是一个参数化的非线性函数$\phi(w_t,h_{t-1})$，循环地将一系列向量映射到一系列隐藏状态。 将$\phi(w_t,h_{t-1})$函数应用于任何这样的序列，在每个时间步骤$t$产生隐藏状态$h_t$，如下所示：
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
  h_t \leftarrow  \phi(W\theta^w_t + U h_{t-1} +c),
\end{equation}
其中$ W,U,V $是模型参数，向量~$\theta^w_t$~对应于源词~$w_t$~的词向量。参数~$U$~是随时间共享的，即：当我们需要计算 $h_t$，我们需要用到上一步的隐藏层的输出$h_{t-1}$。

自Elman提出基本结构~\upcite{DBLP:journals/cogsci/Elman90}~以来，许多扩展模型能学习长距离依赖关系，解决梯度消失（Gradient Vanishing）和梯度爆炸（Gradient Exploding）等问题， 如长短期记忆网络（LSTM）\upcite{7508408}、门限记忆单元（GRU）\upcite{DBLP:conf/nips/ChungKDGCB15}和近似递归神经网络（Quasi-RNNs）\upcite{DBLP:journals/corr/BradburyMXS16}。

\begin{figure}[!b]
  \centering
  \includegraphics[width=.8\linewidth]{./figures/lstm.pdf}
  \caption{LSTM 模型示意图}\label{fig:lstm}
\end{figure}

长短记忆网络（LSTM）是根据RNN 所改进的最著名的模型之一，LSTM 模型的计算公式定义如下\upcite{DBLP:journals/neco/HochreiterS97}： 1）输入门$i_t$ 控制当前输入 $x_t$ 和前一步输出 $h_{t−1}$ 进入新的 Cell 的信息量；2） 遗忘门$f_t$ 决定是否清除或者保持单一部分的状态；3） 变换输出和前一状态到最新状态~$g_t$；4） 输出门$o_t$计算 Cell 的输出 ；5） cell 状态更新$s_t$。其计算步骤是，计算下一个时间戳的状态使用经过门处理的前一状态和输入；6） 最终 LSTM 的输出$h_t$ 使用一个对当前状态进行 $\tanh$ 变换。其计算公式如下：
\begin{equation}\label{equ:lstm}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{split}
   i_t&=\sigma(W^i x_t+U^i h_{t-1}+b^i) \\
   f_t&=\sigma(W^f x_t+U^f h_{t-1}+b^f) \\
   g_t&=\phi(W^g x_t+U^g h_{t-1}+b^g) \\
   o_t&=\sigma(W^o x_t+U^o h^{t-1}+b^o) \\
   s_t&=g_t\odot i_t+s_{t-1}\odot f_t,\quad h_t=s_t\odot \phi(o_t)
\end{split}
\end{equation}
其中$\odot$ 代表对应元素相乘（Element-wise Matrix Multiplication）， 函数 $\phi(x), \sigma(x)$ 的定义：
\begin{equation}\label{equ:tanh}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
  \phi(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}},\sigma(x)=\frac{1}{1+e^{-x}}
\end{equation}

在具体算法实现过程中，LSTM模型由于参数之间相似性和独立性，我们可以并行计算实现该模型。具体来说，模型的输入门、遗忘门、输出门和状态更新门所涉及的矩阵乘法操作我们可以同时计算，从而提高LSTM模型的计算效率\upcite{陈凯2016深度学习模型的高效训练算法研究}。因为LSTM的计算模型更容易并行，所以他与GRU的计算时间相差无几，尽管LSTM的模型需要的参数量是GRU模型的1.5倍。
\begin{equation}\label{equ:weight}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
[i^t,f^t,g^t,o^t ]=[\sigma, \sigma,\phi,\sigma]\times W\times[x_t,h_{t-1}]^\top
\end{equation}
其中 $W$ 指的是8个小矩阵互相层叠起来，即 $[[W^i,W^f,W^g,W^o],[U^i,U^f,U^g,U^o]]$。


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.6\linewidth]{./figures/gru.pdf}
  \caption{GRU模型示意图}\label{fig:gru}
\end{figure}

接下来我们介绍基于LSTM简化的模型，门限记忆单元（GRU）把 LSTM模型中的遗忘门$f_t$和输入门$o_t$用更新门$z_t$ 替代。同时，它把 cell的状态值和隐状态 $h_t$ 进行了合并。 图~\ref{fig:gru}~是GRU更新 $h_t$ 的过程\upcite{DBLP:journals/corr/Pezeshki15}, 主要包括：更新门 $z_t$、 重置门 $r_t$ 和节点内部更新值$\tilde h_t$和隐藏层输出值$h_t$， 这里与LSTM的区别是GRU中没有输出门，其计算公式如下：
\begin{equation}\label{equ:gru}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{split}
   z_t &= \sigma ( W^z x_t+ U^z h_{t-1}) \\
   r_t &= \sigma(W^r x_t  + U^r h_{t-1}  )\\
   \tilde h_t  &= \tanh (W^h x_t  + U^h(h_{t-1} \odot r_t) ) \\
   h_t &= (1-z_t)\odot \tilde h_t  + z_t \odot h_{t-1}
\end{split}
\end{equation}

如果重置门$r_t \to 0$，那么计算 $h_t$时将会丢弃之前的所有信息，即：$\tilde h_t=\tanh W^h x_t$； 若$z_t\to 1$，直接把前一刻隐藏层输出复制到当前时刻$ h_t =h_{t-1}$。 一般来说那些具有短距离依赖的单元重置门比较活跃（如果$r_t=1,z_t=0$， 在这种情况下，GRU模型变成了一个最传统的RNN模型，它只能处理短距离依赖问题），具有长距离依赖的单元更新门比较活跃。



\section{大词表问题}
softmax函数和log-softmax函数通常用于多标签分类（Multi-class Classification）问题中的标准概率归一化（Probability Normalization）。然而当我们需要分类的目标变多时，即对于语言模型的大词表问题中，softmax函数会占用绝大部分计算时间，需要我们对这个计算瓶颈做更详细分析。这里我们首先给出 softmax，log-softmax 函数和相应的梯度（Gradient）的定义：
\begin{equation}\label{eq:softmax}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{split}
p(w|h)=&\frac{\exp(h^\top v_{w})}{\sum_{w_j\in \mathcal{V}}{\exp(h^\top v_{w_j} )}} \\
\frac{\partial p(w_i|h)}{\partial v_{w_j}}=&p(w_j|h)(\delta_{ij}-p(w_i|h))h^\top\\
\end{split}
\end{equation}
\begin{equation}
\label{eq:log-softmax}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{split}
\log p(w|h) &= \theta^w h-\log \sum_{u\in \mathcal{V}}{\exp(\theta^u h)}\\
\nabla_{\theta^u}{\log p(w|h)}&= (\delta_{uw}-p(w|h))h
\end{split}
\end{equation}
其中~$h$~指代的是隐藏层输出的向量（即上下文表示），$\theta^w$~表示单词$w$的目标单词向量~\upcite{duda2012pattern}。 同样对于公式中出现的克罗内克 $\delta$ 函数$ \delta_{uv}$（Kronecker Function），它的定义为：如果$ u,v $指代的是相同的单词结果是$ 1 $，如果$ u,v $指代的不是相同的单词则是$ 0 $。第一行计算公式指代的是softmax函数，第二行计算公式指代的是softmax函数的导数，第三行计算公式指代的是log-softmax函数，第四行计算公式指代的是log-softmax的导数。


如公式~(\ref{eq:softmax})~和~(\ref{eq:log-softmax})~所示，前向概率传播函数$\log p(w|h) $（即第一行方程）和后向梯度优化函数${\partial p(w_i|h)}/{\partial v_{w_j}}$（即第二行的方程）都需要计算目标词汇表中的所有单词，因此在这个部分上花费的时间会随着词表中包含的单词数量的增加而线性地增长~\upcite{DBLP:conf/acl/ChenGA16}。举个例子来说，假设词表中包含$ \mathcal{| V |} $ 数量的单词来说，总的时间复杂度（Time Complexity）是$\mathcal{O}(\mathcal{|H ||V|})$，其中$\mathcal{|H|}$表示的是隐藏层输出的维度。对于基于CPU设备的线性计算设备来说，该部分计算占用总计算时间也是足够大的。
若我们将该部分计算迁移到GPGPU设备上，他将求解过程分解为：计算矩阵乘法（Multiply），计算求和函数（Summary），计算概率归一化的除法函数（Divide）。第一步求解操作占用了主要的计算消耗，即使对于采用现代通用计算体系结构的GPGPU来说， 尽管其非常适合于具有高计算并行的矩阵乘法，这种计算负担也是相当高的。因此，采用现代硬件体系结构的训练和推理中，输出词向量矩阵的超大尺寸仍然是一个计算瓶颈。而且，目前的GPGPU设备的显存（Graphical Memory）是有限的，通常Nvidia 公司提供的设备为12GB显存，目前最大的计算设备Nvidia-K80显存为24GB，然而这个设备造价相当昂贵\footnote{https://www.nvidia.com/zh-cn/data-center/tesla/}。因为我们不能任意增大显存大小，所以对词表大小有一个天然的上限。

需要注意的是，softmax 概率计算瓶颈是求和函数还是矩阵乘法函数？ softmax 算法的瓶颈不能归因于循环函数（即方程~\ref{eq:softmax}~）中的$ \sum_ {u \in \mathcal {V}} $，尽管它随着词汇量的增长而线性增长，但是矩阵张量的乘法计算的规模更大。因为矩阵计算需要更多的时间来获得结果，但线性求和可以更快地计算完成。所以，相比于``for''循环函数而言，主要的计算时间用于矩阵乘法计算。我们在这里说明的目的是：只有那些避免大矩阵乘法运算，即避免计算其他冗余字的计算概率才能达到实际最大的边际加速比，而那些仍然涉及全局概率归一化的方法，实际上并不能真正改善这个部分的计算占用的时间，那些试图优化求和函数的计算效率所获得的收益更是微乎其微。



因此，在保证模型计算准确度不下降的情况下，我们期望能缓解概率归一化函数的计算瓶颈进而提高模型的训练速度。 目前能有效缓解大词表问题的算法主要分为以下三类： 单词拆分算法（Vocabulary Truncation）、采样估计模型（Sampling-based Approximation）和词表层次分解（Vocabulary Factorisation）。下面将对三种不同思路的算法进行详述。


\subsection{单词拆分算法}
针对大词表问题的解决方法，最直接、最简单的策略就是我们放弃使用大词表，转而保留一个较小的词表来保证训练和测试的时候模型的内存（Memory Footprint）占用小和计算效率高。当我们使用较小的词表来保留高频词之后，就会剩下很多其他不在词表里面的单词被映射成$unk$。这相当于我们直接丢弃和模型的输入信息，可能使模型的性能变差。那么针对剩余的过多的词表外的单词（Out-Of-Vocabulary，OOV）， Schwenk~等人提出可以使用传统的N-gram语言模型来估算其可能的概率分布~\upcite{DBLP:journals/csl/Schwenk07}。
这样做一方面保证神经网络模型可以在有限时间内训练完，同时保证模型的最后的测试结果不会很差。

然而我们需要注意到，当我们的词表继续减少并且我们选取的训练样本的词表包含的所有单词数量不断增加的时候，我们会发现训练样本中存在非常多的$\langle$unk$\rangle$ 字符，这直接导致我们转换后的句子变成了无法读或者理解的句子。当我们肉眼可见的句子是无法理解的时候，已经导致输入模型的信息噪声非常大，这样使得神经网络的模型训练非常困难，进而导致模型效果变得非常地差。所以这种方案只是一定程度上缓解了大词表问题，但是不失为一种简单且有效的尝试方案。

除了上述直接的建模方案外，目前采用的方案是将单词按照字符级别来划分。英文单词是由多个字符（Character）组成的，可以将一个单词按照字符统计规律划分成任意多个子词（Sub-word）。例如，Tucker等人提出利用二元对编码（Byte Pair Encoding，BPE）能将一个单词划分成两部分：前缀（Prefix）和后缀（Suffix）~\upcite{DBLP:conf/icassp/Tucker0P94, DBLP:conf/acl/SennrichHB16a,Gage:1994:NAD:177910.177914}，如图~\ref{fig:subword} 所示。从该图中我们可以看出，单词被划分成前缀和后缀两部分，其中$+$代表是单词的前缀，同时$\langle /w \rangle$是单词的后缀。由于划分规则是从训练数据中学习到的，我们可以指定需要缩减的词表大小，所以该算法的动态适应性很强。最初Kucker等人提出该算法目的是问了方便文本信息压缩（Zip软件），后来~Sennrichr~等人将该算法应用在目前主流的机器翻译模型中，获得很好的精度和性能收益~\upcite{DBLP:journals/corr/JozefowiczVSSW16}。后续的提出的算法中，直接将单词拆分成字符序列，模型训练得到的将不再是词向量而是子词向量（Sub-word Embedding）或者字符向量（Character Embedding）。在测试的时候，我们将通过组合子词或者字符向量来获得这个单词的实际词向量。尽管如此，我们仍然需要看到它这样将单词解构成前缀和后缀的操作依然带来了一定的损失，因为句子的长度加倍了，对循环神经网络的长距离关系学习能力提出了更高的挑战~\upcite{DBLP:conf/aaai/KimJSR16}。

\begin{figure}[!h]
  \centering
\includegraphics[width=0.56\linewidth]{./figures/subword.pdf}
\caption{词到子词划分样例}\label{fig:subword}
\end{figure}

\subsection{采样估计模型}
第二种策略也是通过减少单词数量从而减少计算量。与上面暴力的将单词拆分成子词不同的是：这种采样算法是在训练的时候，通过采样算法选择少量单词，用来估计softmax函数的可能的概率分布。因为实际只有一个单词是需要我们预测的，其他的单词都是可以认为是一个噪声单词。

目前流行的采样算法（Sampling Algorithm）主要是针对公式~(\ref{eq:softmax})~中对所有单词概率规约那一项进行概率估计，主要有：重要性采样（Importance Sampling）~\upcite{DBLP:journals/tnn/BengioS08}，噪声差分估计（Noise Contrastive Estimation, NCE）~\upcite{DBLP:conf/icml/MnihT12}和Blackout 采样算法~\upcite{DBLP:journals/iclr/JiVSAD15}。第一种算法在Bengio等人的论文实验中被证明模型无法收敛~\upcite{DBLP:journals/tnn/BengioS08}，因此目前主流模型都使用后面的两种采样算法。接下来，我们将会对这两种方法进行更加详细的描述。

对于NCE噪声差分估计算法来说，模型需要学习将正确的单词 $w_0$ 与随机生成的单词 $\{w_1\cdots w_k\}$做一个二元分类。 其中 $w_0$ 指代的是训练样本中真正的下一个单词， $\{w_1\cdots w_k\}$ 是采用先验分布~$q(w)$~产生的随机噪声单词。实际实验中，我们常用的采样的概率分布模型 $q(w)$是单词词频分布（Word Frequency Distribution）。 正例归一化后的概率~$\tilde{p}(y=1|h)$~和所有负例联合概率~$\tilde{p}(y=0|h)$~的公式可以写成：
\begin{equation}\label{equ:nce}
\setlength{\abovedisplayskip}{8pt}
\setlength{\belowdisplayskip}{8pt}
\begin{split}
  \tilde{p}(y=1|h)=&\frac{\exp( \theta^w_0 h)}{ \exp( \theta^w_0 h)+k *q(w_0)}\\
  \tilde{p}(y=0|h)=&\prod_{i=1}^{k}\frac{k *q(w_i)}{\exp( \theta^w_i h)+k *q(w_i)}\\
\end{split}
\end{equation}
需要注意的是噪声概率~$\tilde{p}(y=0|h)$~需要对~$k$~个噪声样本做加和运算而不是对整个词表单词进行的， 因此该算法的计算复杂度就是$\mathcal{O}(k+1)$，与整个词表的大小无关，所以该算法与简单的softmax算法的加速比是$\mathcal{O}(\mathcal{|V|}/(k+1))$。

除了上面这种最经典的算法之外，后续又衍生出了其他的从采样算法。最近才提出的~Blackout~采样算法针对噪声概率归一化的时候与当前上下文的相关，对上述的NCE算法进行了进一步优化和修正~\upcite{DBLP:journals/iclr/JiVSAD15}，同时也对模型引入了更多的计算量。其模型的代价函数计算公式定义为：
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{split}
  \ell=&-\log(p(w_0|h)) - \sum_{w_i \sim q(w)} \log(1 - p(w_i))\\
p(w_i) =& \frac{\exp(\theta^w_i h)}{\sum_{w_i \sim q(w)} \exp(\theta^w_i h)}.
\end{split}
\end{equation}

简短介绍完了两种采样算法，我们来做一个总结。总的来说，这些采样近似算法可以显着加快训练速度，但仍然需要时间来利用单词分布$q(w)$~\upcite{DBLP:conf/naacl/ZophVMK16}来采样大量的噪音单词，采样单词足够多的情况占用的时间仍然很客观，需要进一步的手段去优化。 另一方面，Softmax函数在推理测试的时候必须要计算，意味着在测试推理的时候该算法失效了。因为候选词是在整个词汇表中预测的，而不仅仅对当前的采样的早噪声单词做预测，而且我们更无法得知正确的单词是什么\upcite{DBLP:journals/jmlr/GutmannH12}。

这里我们需要注意的是，由于我们在做采样估计的时候引入了采样函数$q(w)$，模型将会花费额外时间在采样计算上面。从离散分布采样的初始程序通常需要词汇数量具有线性时间复杂度。因此我们需要寻找一个性能优异的带权重的随机数生成器（Weight Random Number Generator）或者是离散采样函数（Discrete Sampling Function）。


\subsection{词表层次分解}
介绍完单词拆分、采样估计算法之后，我们再从模型角度分解优化这个大词表问题。层次分解方法（Factorization）可以大大降低学习和推理过程中表示概率分布的所占用的计算内存，因为它只计算局部概率（Local Probability）并且选择每一层的分数最高的候选路径而不是保存全局计算结果。
目前主要的分解策略可以分为： 基于类别的多元分类模型（Class-based Hierarchical Softmax, cHSM）和基于二叉树的二元分类模型（Tree-based Hierarchical Softmax, tHSM）。接下来我们将介绍这两种算法的计算过程。

首先我们考虑基于类别的分解策略。假设我们将文本所对应的词表中的每一个词划分成各个类簇，每个单词属于且只属于一个类（Class或者Group）。在此假设之上，我们进而来计算单词在在语料中的对数分布时，可以转化为：先计算类的概率分布，然后乘以在所属类上该词的概率分布\upcite{王龙2015基于循环神经网络的汉语语言模型并行优化算法}~，于是可将公式~(\ref{eq:softmax})~转化为如下形式：
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\begin{split}
p(w|h)=&p^c(\mathcal{C}(w)|h)\cdot p^w(w|\mathcal{C}(w),h) ,\\
 & w\in \mathcal{C}(w),\mathcal{V}=\bigcup _{i = 1}^\mathcal{C}{c_i}, \quad  c_i \bigcap c_j=\phi, \text{若}\quad i\ne j, \\
\end{split}
\end{equation}
其中第一项概率~$p^c(\mathcal{C}(w)|h)$~表示每个类别的概率，第二项概率~$p^w(w|\mathcal{C}(w),h)$~表示在类别$\mathcal{C}(w)$中所有单词的局部概率。

该模型计算一个词的概率分布所需要的计算复杂度正比于: $\mathcal{O =|H||C|}$。 在该计算公式中，$\mathcal{C}$~表示的是该语料中所有词的分类数目，我们可以按照不同策略划分词表，最简单的一种策略就是根据语料中词的词频进行均匀划分。 当$C$ 取$1$ 或取词典大小$|V|$ 时，此结构等同于标准的Softmax 结构，此时词表还是原来的结构，并没有任何优化效果。因此，对于绝大多数情况，我们取$\mathcal{C} \ll \mathcal{V}$并且 $|\mathcal{C}|\ll 1$，这样的结构才能保证有效降低 softmax 的计算复杂度。图~\ref{fig:case_hsm}~展示了类别划分和分步概率计算的示例。从图~\ref{fig:case_hsm}~中我们可以得知，词表 [duck,cat,mop,broom,the,am] 被划分成三个类别：$c_1\to$[duck,cat]，$c_2\to$[mop,broom]并且$c_3\to$[the,am]。
\begin{figure}[!t]
  \centering
\includegraphics[width=.9\linewidth]{./figures/case_chsm.pdf}
\caption{cHSM算法可视化模型}\label{fig:case_hsm}
\end{figure}

接下来我们来分析一下基于类别的分层模型计算速度提升效果。假设每个类包含 $\sqrt{\mathcal{|V|}}$ 单词，表示词汇被划分为相同大小的组，这样的话级联概率（Cascaded Probability）只涉及$2\sqrt{\mathcal{|V|}}$ 单词 softmax计算，所以最优时间复杂度可以减少到$\mathcal{O}(\mathcal{|H|}\sqrt{\mathcal{|V|}})$~\upcite{DBLP:conf/icassp/Goodman01}。虽然在更常见的情况下，词表划分算法会产生不同大小的分组，这需要外部的其他结构来建立高效的数据结构，并且这些稍微复杂的算法尚未被完全探索和分析。此外，我们还可以通过不同的类大小和分区算法来调整精度和效率之间的权衡关系。

另一方面，我们再介绍基于二叉树分解词表的建模策略。tHSM方法将单步骤多类分类分解为多步骤二元分类步骤。正因为如此，词汇组织为二叉树，其中单词分布在二叉树的叶子节点上，二叉树的所有的中间节点的都是内部参数。在该二叉树是平衡树（Balanced Binary Tree）的结构情况下，每个词将会被赋予相等长度的01前缀，最优时间复杂度可以减少到$\mathcal{O(|H|\log \mathcal{|V|})}$。图~\ref{fig:case_thsm} 展示了一颗平衡二叉树词表分解和访问路径的示例。关于如何构建这颗二叉树，最早的工作中，单词在二叉树上的分布可以由WordNet与人类专家~\upcite{DBLP:conf/aistats/MorinB05}或单词 Unigram 分布~\upcite{DBLP:conf/nips/MikolovSCCD13}的霍夫曼编码构建。然而，在现实世界的大规模文本应用中，专家知识的构建是相当昂贵的。所以，一般人们也不会采用这样的方案来构建单词的二叉树分布。霍夫曼编码方案只考虑单数据统计，而单词的语境，句法或语义信息尚未被考虑和得到彻底的讨论。

\begin{figure}[!t]
  \centering
\includegraphics[width=.85\linewidth]{./figures/thsm-example.pdf}
\caption{tHSM算法可视化模型}\label{fig:case_thsm}
\end{figure}

Mikolov~等人当年曾提出使用基于二叉树的层级softmax模型来加速的训练方案，加速比能达到理论的最大速度。但是需要注意的是，当时提出的背景是基于CPU计算方案构建的，如今随着应用领域的推广越来越多的算法应用到实际问题中，我们需要在并行吞吐量更高的GPGPU上进行计算，因此基于GPGPU进行建模的~tHSM~算法和历史上研究众多的cHSM算法尚未被研究提及，因此我们将会需要在本研究中详细研讨。

\section{离散采样算法}
\begin{figure}[!b]
  \centering
\includegraphics[width=1\linewidth]{./figures/cdfreverse.pdf}
\caption{由离散概率分布构造累积概率分布}\label{fig:cdf_reverse}
\end{figure}


前面我们提到在使用基于采样的模型时，我们不可避免地需要使用到离散采样函数，我们在这一个部分来讨论离散采样函数的构建策略。我们首先给出离散采样问题的定义：设一共有$|\mathcal{V}|$个单词，第$i$个单词$w_i$出现的概率是$p(w_i)$， 如何高效地产生这样的随机变量序列$w_1,w_2,\cdots$?这个问题被称之为模拟离散取样（Simulated Discrete Sampling），其含义指的是根据有限类别的非均匀概率分布，来模拟有放回的采样，从而生成我们需要的采样序列。因为我们的计算机只能快速产生伪随机数（Psudo Random Number Generator），即对均匀分布（Uniform Distribution）做采样。其他的复杂采样函数均建立在均匀分布的变换之上。所以该问题的关键就是如何通过将均匀分布变换到我们实际变量的分布，生成指定概率分布的随机单词序列。

如图~\ref{fig:cdf_reverse}~所示，我们首先将离散概率分布转换成累积分布函数表（Cumulative Distribution Function，CDF）。我们可以使用Matlab代码，很容易地完成这个计算操作。如图~\ref{fig:sample}~所示，所展示的代码使用了线性搜索算法(Linear Search)。该算法很简单，最容易实现。但是我们注意到该算法初始化需要消耗$\mathcal{O(\mathcal{V})}$时间复杂度。计算每个样本时候的时间复杂度也是$\mathcal{O(V)}$。
\begin{figure}[!t]
  \centering
\includegraphics[width=1\linewidth]{./figures/cdf.pdf}
\caption{Matlab实现的离散采样函数}\label{fig:sample}
\end{figure}


考虑到该CDF表格是从小到大的顺序结构，我们就可以使用数据结构中的查找算法更快地找到结果。我们可以采用从左至右的方式线性搜索随机数所在的区间其时间复杂度是$\mathcal{O}(\mathcal{V})$，也可以使用更高效的二叉搜索(Binary Search)快速计算随机数所在的区间对应的时间复杂度是$\mathcal{O}(\log \mathcal{V})$。



在$\mathcal{V}$很小的情况下，使用线性查找算法或者二分搜寻算法也足够得快；当$\mathcal{V}$足够的大，即所谓的大词表问题的时候，我们可以采用别名方法(Alias Method)\upcite{DBLP:reference/stat/LEcuyer11}、近似方法等~\upcite{DBLP:journals/cgf/ClineRW09}。本实验用到了别名算法，它利用两次随机采样来生成采样序列，具有恒定的~$O(1)$采样时间复杂度。该算法的精髓在于构造别名表，需要~$O(\log |\mathcal{V}|)$构造时间~\upcite{10.2307/2683739,DBLP:conf/emnlp/VaswaniZFC13}，具体构造方法可以参考\footnote{https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/}。

\begin{figure}[!t]
  \centering
\includegraphics[width=1\linewidth]{./figures/alias.pdf}
\caption{由离散概率分布构造别名表}\label{fig:alias}
\end{figure}

\section{本章小结}
本章对国内外学者在语言模型的任务上相关工作进行了介绍。首先，我们介绍了语言模型的建模策略和对应的应用方案。其次详细讨论了循环神经网络的语言模型的建模方式，并讨论了其结构的优缺点。同时为了解决使用循环网络所带来的梯度爆炸问题，我们进而引入了长距离短期神经网络和门限记忆节点两种基于门限机制的循环网络；另一方面，我们还考虑了语言模型大规模应用所带来的大词表问题，并作了详细分析原因。
具体来说，我们还讨论了历史上所提出的解决方案，并做了分类讨论。
主要涉及三种思路：缩减词表大小，用采样算法近似估计和模型层次分解。针对各种模型提出的结构做了介绍，并分析了各种算法的优劣处。最后我们分析了实验中需要用到的高效采样函数的构造算法。