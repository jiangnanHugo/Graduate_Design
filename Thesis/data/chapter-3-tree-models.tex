\RestyleAlgo{boxed}
\chapter{并行树状概率计算模型}
\section{引言}
在本章中，我们将这个预测端的词用二叉树的分层的形式表示，使它们的编码适配基于树的分层概率计算。首先，我们提出了一个在分层结构上建模参数的字编码方案。因为考虑到GPU上的算法的并行计算的吞吐性能，我们进而推导出紧凑的代价函数及其梯度，使得模型在GPU设备上的能并行计算模型的内部概率函数。

同时，树上的单词分布对其性能有很大的影响，应该在训练阶段之前定义。在本实验中，我们并不打算考虑那些在训练过程中动态交换子树的算法进而改变了单词在子树上的分布结构， 而是我们采用了几个分层聚类策略，用统计，句法和语义知识来初始化其结构，以达到一个稳定和可以预期的性能。

另外，在模型的测试推理过程中，不同于传统的softmax算法情况，得到最好的候选者自然是可行的并且能直接被计算出来，层次推理模型不能直接应用 softmax 方法来实现。因此我们讨论了基于二叉树的搜索策略，以满足两种不同的推理情情形：a）给句子打分。输出给定序列的概率；b）给定上下文，对文本进行排序。在给定的上下文中获取得分最高的一个候选单词。
\section{基于二叉树的单词极性编码}
首先，所有的单词分布在二叉树的叶节点上，因此我们可以通过访问从根到叶节点的所有内部节点来定位每个特定的词，所以不同的访问路径代表不同的单词。在这里，单词$ w $的路径表示所有内部节点$ \theta^w_i $和它访问的边$ d ^ w_i $。

\begin{table}[!ht]
  \centering
  \caption{并行树状概率计算模型的符号助记表}
\begin{tabular}{llc}
  \toprule
   符号&涵义&取值范围\\ \midrule
$l^w$ &单词$w$ 所对应的叶子节点和中间节点的路径&Int32 \\
$d_j\in \{-1,+1\}$&表示路径$l^w$中第$j$个节点对应的编码（根节点对应编码$0$）&\\
$ d^w=[d_0^w,d_1^w,\cdots,d_{l^w-1}^w] $& 单词 $w$ 的极性编码，他由 $l^w-1$位编码构成，&\\
$\theta_{j}^w\in\mathbb{R}^m$ &表示路径$l^w$中第 $j$ 个非叶子节点对应的向量& Float32\\
$ \theta^w=[\theta_1^w,\theta_2^w,\theta_3^w, \cdots,\theta_{l^w}^w]$&表示路径$l^w$所对应的参数矩阵&Float32 \\
$\Gamma$ &路径查找表，给定路径序列，可以获得对应的单词& \\
  \bottomrule
\end{tabular}
\end{table}

我们接下来详细说明模型的编码方式。其中，$ \theta_i ^ w $表示到达单词$w$的路径上的$ i^{th} $层上的非叶节点，并且$\theta_i ^ w$是一个 $m$维度的向量，$ \theta_i ^ w \in\mathbb{R}^m $ 其中$ i \in [0, l ^ w - 1] $。同样的，$ d_i ^ w $表示连接第$(i-1)^ {th} $和$ i ^ {th} $层节点的边。对于每个非叶子节点来说，向下移动到左边的分支标记为$ -1 $，选择正确的分支标记为$ + 1 $。因此，在$i\in[0,l^w-1]$, $d_i^w\in \{-1,+1\}$。 此外， $l^w$~表示从根到叶单词的路径长度。如果该二叉树是平衡树，即单词都均匀分布在同一层叶子节点上，那么树深是$l^w\approx \log \mathcal{|V|}$ 。通过这个方案，我们可以通过表示极性路径来定位每个单词，将单词索引（Indexing）或二元稀疏表示（One-hot Representation）改变为单词极性编码元组$(d ^ w,\theta ^ w)$。


在用python语言实现过程中，我们通过维护一个路径查找表$\Gamma$（Lookup Table），记住每个单词$ w $从根到叶的所有访问内部节点的索引。这样，通过从$ \Gamma(w)$中选择对应行的所有节点，从参数矩阵${\Theta} $中检索$ \theta ^ w $。其中${\Theta} $的第一维的维度是：
\begin{equation}\label{equ:sums}
\sum_{i=0}^{\log \mathcal{|V|}}{2^i} = \mathcal{|V|} -1.
\end{equation}
因此，p-tHSM模型没有增加模型额外参数，除了预先给定的路径查找表$ \Gamma $。

除此以外，我们通过从矩阵$\mathcal{D}$中获得$w^{th} $行向量来检索$d^w$，其中$w$是词汇表中的单词索引。 此外，$\{\Gamma,\mathcal{D}\}$ 是由层次聚类算法预先给定的，$\Theta$是通过训练数据集上的代价函数的梯度下降来优化的。为了保证理解正确，我们在这里再一次强调$ \Theta $是模型的参数，$ \Gamma $记忆路径节点索引信息，$\mathcal {D}$取 $ \{ - 1,+1 \} $中的值。

如图~\ref{fig:tree_hsm}~所示，内部参数向量 $\theta_i^w$, 边 $d_i^w$ 并且单词在树的叶子节点上。除此之外, 加粗的那条路径从根节点到叶子节点 $w$ 被定义为参数对 $(d^w,\theta^w)$。其中 $d^w$ 是一个向量， $\theta^w$ 是一个参数矩阵. 例如, 图里面的参数实际结果是 $d^w=[-1,+1,-1]$ ， $d^{v}=[-1,+1,+1]$。
\begin{figure}[!h]
  \centering
    \includegraphics[width=0.8\linewidth]{./figures/thsm.pdf}
\caption{树状层次概率模型}\label{fig:tree_hsm} %
\end{figure}

\section{基于二叉树的代价函数和导数}
在目标词树的每个步骤中，我们对每个非叶节点是左下分支还是右分支进行逻辑预测。 所以当我们给定$ i^{th} $节点和隐藏层$ h $的$ i^{th} $的时候，我们可以计算标签 $d^w_i\in \{-1,1\}$的概率为：
 \begin{equation}\label{equ:pp}
p(d^w_i|\theta_{i}^w,h) =\sigma(\theta_{i}^w h)^{d_i^w}\times(1-\sigma(\theta_{i}^w h))^{1-{d_i^w}},d_i^w \in [0,1]
\end{equation}
其中$ \sigma(z)= 1 /(1 + \exp(-z))$表示 $\sigma$ 函数。根据 $\sigma$ 函数的对称性规则：$\sigma(z)+ \sigma(-z)=1 $，以下是该公式的证明过程：
\begin{equation}\label{equ:sig}
\begin{split}
\sigma(z)+ \sigma(-z)  &=\frac{1}{1 + \exp(-z)}+\frac{1}{1 + \exp(z)}\\
  &=\frac{\exp(z)}{1 + \exp(z)}+\frac{1}{1 + \exp(z)}=\frac{1 + \exp(z)}{1 + \exp(z)}=1
\end{split}
\end{equation}
该规则可以用来帮我们把公式~\ref{equ:pp}~缩写成一下更简洁的形式~\upcite{minka2003algorithms}：
 \begin{equation}
p(d^w_i|\theta_{i}^w,h) =\sigma(\theta_{i}^w h)^{d_i^w}, d_i^w \in [-1,+1]
\end{equation}
由于公式中的其中某一项${d_i^w}$是在指数项上面，所以我们尽管做了这个基于$\sigma$函数的变换，我们的计算公式仍然不是最佳的形式，还是有空间来提升共识的紧凑性保证计算时的高并行度。更进一步的讲，我们可以将上面的公式缩写成以下形式：
\begin{equation}
p(d^w_i=\pm 1|\theta_{i}^w,h) = \sigma({d_i^w}\theta_{i}^w h)
\end{equation}
这样的操作之后，我们就完成了单节点概率计算的紧凑表示。针对计算一个单词的概率，我们需要考虑从根节点到该单词叶子节点的所涉及的所有层的概率公式。然而因为在训练的时候，我们实际上预先知道每个单词所对应的路径，所以我们实际上可以同时的计算各个节点的概率值，然后将所有节点相乘起来。这样做的好处，现在看起来十分费力，但是我们可以在下面详细说明。

 因此，我们传统意义上计算的单词的概率在这里指的是：单词$ w $在二叉树上的概率，即从根到相应叶节点的路由概率$(d^w,\theta^w)$的联合乘积：
\begin{equation}\label{equ:pw}
\begin{split}
 \log p(w|h)=&\log\prod_{i=0}^{l^w-1} p(d^w_i|\theta_{i}^w,h) = \sum_{i=0}^{l^w -1} \log\sigma(d_i^w \theta_{i}^w h)\\
 =&\log\sigma({d^w}^\top \theta^w h)=\zeta(- {d^w}^\top \theta^w h )
 \end{split}
\end{equation}
其中 $\zeta(z)$ 代表 softplus 函数: $\zeta(z)= \log (1+\exp(z))$。 该函数的导数是 $\sigma$ 函数, 其导数计算公式是: ${\mathrm{d}\zeta(z)}/{\mathrm{d} z}= \sigma(z)$~\upcite{DBLP:conf/nips/DugasBBNG00}。如图所示， softplus 函数通常被视为 ReLU函数的替代函数，因为他们两个函数除了零点附近分布不一样以外，其他地方分布相同。但是softplus函数在零点附近可导，ReLU函数在零点附近不可导。
\begin{figure}[!ht]
  \centering
\includegraphics[width=0.6\linewidth]{./figures/relus.pdf}
\caption{Softplus和ReLU函数的示意图}\label{fig:soft}
\end{figure}

接下来，我们给出模型相应的损失函数$ \ell(\theta | h,w)$，它是定义在二叉树上面的负对数似然函数（Negative Log-Likelihood，NLL）：
\begin{equation}\label{equ:cost}
\begin{split}
   \ell(\theta|h,w) =&-\log\prod_{i=0}^{l^w -1} \sigma(d_i^w \theta_{i}^w h) = -\log \sigma({d^w}^\top \theta^w h)\\
    =& \log (1+\exp(- {d^w}^\top \theta^w h )) =  \zeta(- {d^w}^\top \theta^w h )
\end{split}
\end{equation}
从此公式中，我们可以发现最小化基于树的负对数似然值，意味着直接最大化softplus损失和估计字的概率。

尽管如此，在传统的tHSM算法中，该模型逐层计算每个节点的对数概率。 因此，这个词的整体联合对数概率通过各层线性相加。 因此tHSM的时间复杂度为$\mathcal{O(|H|\log|V|})$，其计算公式为：
\begin{equation}
\ell(\theta|h,w) =\sum_{i=0}^{l^w-1} \{(1-d'^w_i)\log (\sigma(\theta_{i}^w h))  + {d'^w_i}\log (1-\sigma (\theta_{i}^w h))\}
\end{equation}
其中 $d'^w_i\in \{0,1\}$，这两个部分从底到顶，分别对内部节点的左右子树的联合概率进行建模。

值得注意的是，p-tHSM和tHSM算法之间的主要区别在于：a）tHSM算法涉及许多微小的矩阵乘法操作，而在p-tHSM中我们直接将所有参数$(d^w,\theta^w)$ 2D矩阵是以运行时内存消耗为代价的，我们考虑这个向量和相对较大的矩阵的乘法，如图~\ref{fig:tree_hsm}~所示。 b）扣除模型的紧凑损失函数，并行的计算这条路径上所有节点的对数概率而不是逐层计算，为p-tHSM模型提供更好的时间效率。

接下来，模型的所有参数$\{\theta^w,h\}$针对模型的代价函数的导数是 ：
\begin{equation}
\begin{split}
\frac{\partial \ell}{\partial \theta^w}=&(\sigma({d^w}^\top\theta^w h) -1){d^w}^\top h \\
\frac{\partial \ell}{\partial h}=&(\sigma({d^w}^\top \theta^w h) -1){d^w}^\top \theta
\end{split}
\end{equation}


二叉树分解的一个主要优点是它避免了在整个词汇表中概率的归一化，因为树中词的汇总概率自然等于1。
\begin{equation}
\sum_{w\in \mathcal{V}}{p(w|h)}=\sum_{w \in \mathcal{V}}\sum_{i=0}^{l^w-1}{\sigma(d_i^w\theta_{i}^w h)}=1.
\end{equation}



\section{基于二叉树的推理算法}
考虑到本章开始提出的第一个问题，即给定序列$ [w_1,\cdots,w_T] $，求该序列出现的概率。 直观地说，当给定相应的上下文$ h $时，我们可以通过获取一个特定单词$ w $的概率或对数概率来分解问题：
\begin{equation}
\begin{split}
    p(w|h) =&\sigma({d^w}^\top \theta^w h)\\
   \log p(w|h) =& -\zeta(- {d^{w}}^\top \theta^{w} h )
\end{split}
\end{equation}
其中概率$ p(w | h)$和单词$ w $的对数概率$ \log p(w | h)$可以直接通过等式~\ref{equ:pw}~和~\ref{equ:cost}~逐步计算出来。 因此，我们可以推导出这个序列的概率为：
\begin{equation}
   \log p(w_1,\cdots, w_T)=\sum_{t=1}^T\log p(w_t|h_t) = -\zeta(- {d^{w_t}}^\top \theta^{w_t} h_t )
\end{equation}
显然，这种类型的操作比传统的softmax方法有效得多，因为它只需要$\mathcal{O}(\mathcal {| H | \log| V |})$计算复杂度，而传统的softmax算法却需要$\mathcal{O}(\mathcal {| H || V |})$。

关于第二种情况（也被称为$\arg\max $方法），即当给定前一个上下文时，在整个词汇表中搜索最有可能的下一个单词（概率最大的单词）。我们可以在选择最上面的一个候选人之前计算词汇表中所有单词的概率。这个过程仍然是昂贵而缓慢的，因为它涉及到整个词的分层树。如在算法~\ref{alog:argmax}~中所描述的，为了避免两个小概率的精确度损失问题，我们选择计算每个内部节点的对数概率。

而不是搜索全局最优结果，我们可以用局部贪婪算法搜索次优结果。具体而言，对于$ i $ -th层中的节点，当$ p(d ^ w_i | \theta_{i} ^ w,h)\ge 0.5 $时选择左边的分支，相反适用，如算法~\ref{alog:greed_argmax}所示。因此，计算时间复杂度仍然是$ \mathcal{O}(| H | \log \mathcal {| V |})$。


\begin{algorithm}[!ht]
\SetAlgoLined
\KwData{隐藏层输出 $h$;}
 路径列表 $\mathtt{route}$=[] \;
\While(\tcp*[h]{逐层贪心路径搜索}){$k \le \log \mathcal{V}$ }{
\eIf{$p(d_{k} |\theta_{k},h) \ge 0.5$ }{
 $k=  k*2$ \tcp*[r]{左分支}
}{
 $k = k*2+1$ \tcp*[r]{右分支}
}
 $\mathtt{route}$.add($k$) \tcp*[r]{将 $k$ 添加到路径列表}
}
{通过查找$\Gamma$路径表，找出对应的单词}\;
$w=\Gamma(\mathtt{route})$\;
\KwResult{ 预测的单词 $w$. }
\caption{逐层贪心搜索算法}\label{alog:greed_argmax}
\end{algorithm}


\begin{algorithm}[!ht]
\SetAlgoLined
\KwData{隐藏层输出 $h$;}
 路径列表 $\mathtt{path}$=[] \;
\While(\tcp*[h]{全局概率计算函数}){$k \le \log \mathcal{V}$ }{
\eIf{$p(d_{k} |\theta_{k},h) \ge 0.5$ }{
 $k=  k*2$ \tcp*[r]{左分支}
}{
 $k = k*2+1$ \tcp*[r]{右分支}
}
 $\mathtt{path}$.append($k$) \tcp*[r]{将 $k$ 添加到路径列表}
}
{通过查找$\Gamma$路径表，找出对应的单词}\;
$w=\Gamma(\mathtt{path})$\;
\KwResult{ 预测的单词 $w$. }
\caption{全局单词最优算法}\label{alog:global}
\end{algorithm}

\section{基于二叉树的聚类算法}
词汇表中每个单词$(d^w,\theta^w)$的极性编码与树的结构密切相关。因此对于提出的p-tHSM算法，我们采用了多种树聚类算法来初始化单词在二叉树上的分布，以提高左后模型的精确性和稳定性。总的来说，这些聚类算法为每个单词生成二进制前缀字符串，表示树中单词的位置，并将用于初始化p-tHSM方法。


1）一元聚类算法。它根据词频对词汇进行排序，并根据单词统计从底端到顶端进行合并，也称为霍夫曼编码~\upcite{DBLP:conf/nips/MikolovSCCD13}。

霍夫曼编码的构造过程。1） 根据$\mathcal{|V|}$个权值$\{w_1,w_2,\cdots,w_{\mathcal{|V|}}\}$，生成$\mathcal{|V|}$棵只有单个结点的树，没有左右子树的二叉树的集合$F\{T_1,T_2,\cdots,T_{\mathcal{|V|}}\}$，其中每颗树的根节点都有一个权值$w_i$；2） 在~$F$~中选取两颗根节点的权值最小的树作为左右子树构造一颗新的二叉树。对于这两颗子树，权值小的作为左子树，权值大的作为右子树。新的二叉树的权值是两颗子树的权值之和；3）将2步选取的两颗树在$F$中删除，并将新二叉树加入$F$中。4）重复2，3步，直到~$F$~中只有一颗树为止。

\begin{algorithm}[!ht]
\SetAlgoLined
\KwData{单词和对应的频度词典 frequenties;}
{Q=PriorityQueue()} \tcp*[r]{初始化优先队列$Q$}
\For(\tcp*[h]{((freq,word),index)}){$v \gets frequenties$}{
{Q.put(v)}\;
}
{$idx=0$}\;
\While{$Q.qsize()>1$}{
    {l,r=Q.get(),Q.get()}\;
    {node=Node(l,r,idx++)}\;
    {freq=l[0]+r[0]}\;
    {Q.put((freq,node))}\;
}
\KwResult{ 霍夫曼树 $Q.get()[1]$. }
\caption{基于单词频率的霍夫曼建树策略}\label{code:huffman}
\end{algorithm}

当我们构造完霍夫曼二叉树后，我们需要遍历这棵树来获取所有单词的极性编码霍夫曼编码：1）计算待压缩数据中所有的字符及其出现次数，根据次数的不同对没个字符分配不同的权值（一般用出现频率/总字符数）。2）对所有带压缩字符按起权值，构造一颗霍夫曼树。3）对这棵树所有子树的左支用$-1$编码，右支用$+1$编码。每个位于叶子节点的字符的编码为从根节点到该叶子节点路径上的$-1,+1$编码值,这个在这棵树上重新得到的编码叫霍夫曼编码。这样可以得到每个字符的极性编码。
\begin{algorithm}[!ht]
\SetAlgoLined
\KwData{单词和对应的频度词典 frequenties;}
collector=[], polarity=[], param=[]\;
\If{self.left}{
    \eIf{isinstance(self.left[1],Node)}{
          {self.left[1].preorder(polarity+[-1],param+[self.index],collector)}\;
    }{
          {collector.append((self.left[1],param+[self.index], polarity + [-1]))}\;
    }
    }
    \If{self.right}{
        \eIf(\tcp*[h]{右子树存在}){isinstance(self.right[1],Node)}{
          self.right[1].preorder(polarity+[1],param+[self.index],collector)
        }{\tcp*[h]{左子树存在}\;
          collector.append((self.right[1],param+[self.index], polarity + [1]))
          }
    }
\KwResult{ 单词路径查找表 $\gamma$. }
\caption{前序遍历函数生成单词路径查找表}\label{code:preorder}
\end{algorithm}
如图~\ref{fig:zipf}~所示，我们算法中用到输入信息$frequenties$指的是单词的词频分布。
\begin{figure}[!ht]
  \centering
\includegraphics[width=0.6\linewidth]{./figures/zipf.png}
\caption{单词 Unigram分布示意图}\label{fig:zipf}
\end{figure}

其对应的二叉树的数据结构采用基于数组的方式管理和访问。如下所示，每个节点（Node）包含信息为：左子树，右子树，节点编号。同时我们还定义了节点的打印字符串函数（即``repr''），方便我们检查数据结构是否正确实现。
\begin{minted}[mathescape,linenos=False,gobble=1,frame=lines,baselinestretch=1.0,fontsize=\footnotesize,framesep=1mm]{python}
 class Node(object):
  def __init__(self,left=None,right=None,index=None):
   self.left=left
   self.right=right
   self.index=index

  def __repr__(self):
   string=str(self.index)
   if self.left:
    string+=', -1:'+str(self.left.index)
   if self.right:
    string+=', +1:'+str(self.right.index)
   return string
\end{minted}


2）二元聚类算法\footnote{布朗聚类：https://github.com/percyliang/brown-cluster}。它是一个层次聚类算法，使用bigram上下文来确定单词的分布相似性，将相似的单词放置在二叉树的附近位置~\upcite{DBLP:journals/coling/BrownPdLM92,liang2005semi}。在将簇大小指定为1之后，它从底部到顶部合并具有一个节点的单词。生成的单词二进制路由正是分层二进制结构的分布。
\begin{figure}[!ht]
  \centering
\includegraphics[width=0.6\linewidth]{./figures/brown.png}
\caption{布朗聚类分布示意图}\label{fig:brown}
\end{figure}

3）语义信息聚类算法\footnote{https://code.google.com/archive/p/word2vec/}。在引导式的方式下，词嵌入在外部语料库上训练，我们将传统的凝聚式聚类应用于字嵌入特征~\upcite{DBLP:books/sp/mining12}。


\section{本章小结}
本章首先定义了极性编码的概念，同时给出了模型所涉及的参数的详细涵义。接下来，我们逐步推导模型的单个节点的概率公式，单个词的概率公式和模型的代价函数。另一方面，我们将提出的p-tHSM算法和传统的线性tHSM算法进行的比较。通过比较两者计算的差异性证明我们提出的算法更适合在GPU等高并行设备上运算。进一步的，我们还讨论了模型在测试的时候所需的推理算法，因为基于二叉树的概率计算方案和传统的softmax计算方案不同，不能直接输出单个词的概率或者计算最佳的候选单词，所以我们分别针对这两个任务提出推理算法。最后，由于单词在二叉树上的分布需要初始化，我们讨论了传统的霍夫曼聚类算法，布朗聚类算法和语义向量聚类算法，并给出了三种算法的详细计算过程。
