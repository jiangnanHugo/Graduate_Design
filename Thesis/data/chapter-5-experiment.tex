
\chapter{语言建模实验结果与分析}
前面两章分别介绍了基于树状和类别的层次概率模型的具体建模过程。在本章中，为了比较所提出的这两种分层概率模型与已有的算法（Baselines）在计算效率和精确性上的差异，我们将在三个标准文本数据集上进行循环语言建模任务的实验研究和结果分析。本章将讨论分析不同并行层次概率算法的实验结果，还会与其他大词表问题的优化加速算法相互对比。 接下来还将从效率、可扩展性、参数大小等方面，对这些已有的方法进行实验研究并作讨论分析。

\section{实验设置}
这一小节将介绍实验所需要用到的文本数据集，还有语言建模实验的两种主要的评测指标和实验模型的实际参数配置和训练过程。
\subsection{实验数据集}
本实验所采用的数据集主要是依照两个目的选取的：1）首先为了便于实验复现和互相对比，需要在小数据集上反映出参数变化对模型的最后的效果产生的影响；2）同时还需要大数据集上，展示模型参数在最佳参数配置下，比较模型之间的最优结果。所以在本次实验中，我们选取了三个标准文本数据集：Wikitext-2，Wikitext-103 和 One Billion Words数据集。
如表~\ref{tab:dataset}~所示，表中列举了这三个数据集的相关统计指标，包括：单词数量、句子数量、词表大小和词表外单词的比例（Out-of-Vocabulary）。需要注意的是，词表大小是不能改变，因为不同词表规模的模型之间理论上是没有互相比较的意义，其次是由于接下来要计算的一个重要的评测指标和词表大小成负相关关系。
所以表~\ref{tab:dataset}~中展示的数据已经预先固定了，不会再做任何的修改，例如：不能对数据集做单词大小写变化、数词转换操作或者分词操作。
\begin{table}[!ht]
  \centering
  \caption{WikiText-2, WikiText-103 和 One Billion Words 数据集统计指标 \label{tab:dataset}}
\begin{tabular}{llrrrrr}
\toprule
数据集& 类型& 文章数 & 句子数量 &  单词数量 &词表大小 & OOV （\%） \\ \midrule
\multirow{3}{*}{Wikitext-2} &训练集& 600 & 36,718 & 2,088,628 & \multirow{3}{*}{33,278} & \multirow{3}{*}{2.6\%} \\
&验证集& 60 &3,760 & 217,646  & &\\
&测试集& 60 & 4,358 & 245,569 & &\\
\midrule
\multirow{3}{*}{Wikitext-103} &训练集& 28,475 &  1,801,350 &  103,227,021 & \multirow{3}{*}{267,735} & \multirow{3}{*}{0.4\%} \\
&验证集& 60 &3,760 & 217,646  & &\\
&测试集& 60 & 4,358 & 245,569 & &\\
\midrule
\multirow{3}{*}{One Billion Words} &训练集& --- &30,301,028&768,646,526&   \multirow{3}{*}{793,471} &   \multirow{3}{*}{0.28\%} \\
 &验证集& --- &  6,075 &   153,583 &&\\
 &测试集 & --- &  6,206 &   159,354 &&\\
\bottomrule
\end{tabular}
\end{table}

其中，对于~Wikitext-2和Wikitext-103~数据集，训练、验证和测试集都是预先划分固定的，并且其词汇表大小也已经被定义\footnote{https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/}。
这两个数据集拥有相同的验证和测试集，而Wikitext-103的训练集则比Wikitext-2的训练集大得多。所以我们还可以间接测算出增大训练集对测试集上的提升效果。
对于第三个``One Billion Words''数据集\footnote{http://www.statmt.org/lm-benchmark/}，它是由历史机器翻译数据集积累的文本融合而成，文本数据也取自维基百科（Wikipedia）\footnote{https://en.wikipedia.org/wiki/Main\_Page/}。
为了保证评价的标准性和实验的可互比性，官方还提供了一套数据预处理脚本，同时指定了该脚本所需要的~Perl~语言版本，可见要求极为严苛。
因为对于语言模型来说，Perl~内置函数版本不同，处理出来的文本也略有不同，语言模型之间的结果就不在同一个基准之上。通常来说，词表小的模型更占优势。当用官方提供的脚本处理完数据后，我们将``./train/''目录中的所有数据视为训练集，选择``./holdout/''目录下面的第一和第二个数据集作为相应的验证和测试集。这些数据集的详细统计指标在表~\ref{tab:dataset}~中进行了说明。

\subsection{实验评价指标}
在本次实验研究中，当实验模型在验证数据集上训练到收敛后，我们就需要评价不同模型的性能差异，针对语言模型的两个标准评估度量标准来揭示针对不同的优化方法的优劣：困惑度（Perplexity，$ \mathrm{PPL} $）和单词误差率（Word Error Rate，$\mathrm{WER} $）。
其中，$ \mathrm{PPL} $是一个内在度量指标（Intrinsic Metric），代表了在不同语境中选择下一个候选单词时的困惑程度。语言模型困惑度较低，意味着在相同词表下拥有更好的可预测性。此外，在整个测试集上，$\mathrm{PPL}$ 数值是与平均负对数似然值（NLL）呈指数相关关系，这表明训练过程中模型直接优化了~$ \mathrm{PPL} $~评测指标，其数学定义如下所示：
\begin{equation}\label{equ:ppl}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
   \mathrm{PPL}(w_1,\cdots,w_T)=\sqrt[T]{\frac{1}{\prod_{t=1}^T p(w_t|w_{1:t-1})}}
\end{equation}

此外，$\mathrm{WER}$表示单词的萊文斯坦距离（Levenshtein Distance），用于衡量参考句子（Reference，记作r）和预测句子（Hypothesis，记作h）之间的相似度。它是编辑距离的一种衍生类型，被定义为错误识别的单词（删除，插入，替换）占总单词的百分比\footnote{https://martin-thoma.com/word-error-rate-calculation}：
\begin{equation}\label{equ:wer}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
  \mathrm{WER} = \frac{\text{插入的单词数 + 删除的单词数 + 替换的单词数}}{\text{全部单词数量}}
\end{equation}
其数学形式的定义公式为：
\begin{equation}\label{equ:distance}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
d_{r,h}(i,j) =  \begin{cases}
\max (i,j)& \text{如果}\min(i,j)=0\\
\min  \begin{cases}
d_{r,h}(i - 1,j) + 1,\text{//插入的单词}\\
d_{r,h}(i,j - 1) + 1,\text{//删除的单词}\\
d_{r,h}(i - 1,j - 1) + 1\{r_i\neq h_j\},\text{//替换的单词}
\end{cases} &\text{否则}
\end{cases}
\end{equation}
其中$1\{r_i\neq h_j\}$ 指代的是示性函数， 当且仅当$r_i= h_j$的时候取值为$0$，否则该函数取值为$1$。 $d_{r,h}(i,j)$ 表示的是参考句子 $r$的第一个$i$个字符与预测句子 $r$的第一个$j$个字符之间的距离。同时萊文斯坦距离还满足度量的延展性关系（三角不等式），即： 两个字符串的距离不大于分别与第三个字符串的距离之和：
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
d_{r,h}+d_{r,s}\ge d_{r,s}
\end{equation}
其中 $s$ 代表另外一个生成的句子，$d_{r,h}$表示的是句子~r~和句子~h~之间的编辑距离。

除了以上的定量的度量指标外，\textit{训练时间效率，词表可伸缩性}和\textit{运行时内存消耗}等定性度量指标也应该被视为衡量不同模型的重要衡量维度。
因此，我们在实验中分别从~GPGPU~和CPU的实际运算结果分析了不同优化算法在不同评估指标上的具体性能。最后，统计和评测了所有模型在三个标准文本数据集上的最终结果。

\subsection{模型训练和参数配置}
接下来来介绍实验模型的参数设置和训练过程。在具体算法实现中，每个模型都是使用Theano框架实现，而且都运行在一个独立的GPGPU设备上，模型不在多显卡上运算，模型之间不互相干扰。GPGPU设备具有12GB的显存（设备型号是Nvidia K40m），保证能进行大矩阵乘法的运算，所以我们能测算出大词表问题的具体计算代价。然而我们发现，在实际试验中随着模型参数维数的增加，单个GPGPU显存资源被快速利用殆尽。

然后是针对数据集做必要的预处理。对于Wikitext-2数据集，最大句子长度固定为256；对于Wikitext-103数据集，最大句子长度固定为100；对于OBW数据集来说，最大句子长度固定为50，因为第三个数据集的词表最大，需要占用更多的显存资源，所以句子长度有所缩减。对于长度超过阈值的那部分字符串将直接删除，由于测量的是语言模型的单词级损失（Word-Level Loss）而不是句子级的分数（Sentence-Level Loss），因此删除部分的那句子对模型训练的影响是很小的。
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.6\columnwidth]{./figures/learn2.pdf}
  \caption{模型训练曲线图}
\end{figure}

另外在实验中，Adam优化器（Optimizer）配合两种不同的学习率（Learning Rate）作为模型的优化函数，即设置$\mathtt{lr}  = 0.06$ 和 $0.001 $。两种词表层次分解方法收敛速度较慢需要采用较大的学习率，而传统的softmax和采样近似方则需要采用较小的学习率。除此以外，为了减弱模型收敛过程中的损失值的振荡现象，每隔一定步数（$\texttt{Step} =100$），学习率还需要逐渐缩小：$\texttt{lr} \leftarrow \texttt{lr}*0.9$。

对于在Wikitext-2数据集上运行的实验，我们设置批处理大小（Batch Size）为20最大遍历周期（Epoch）为15，直到我们观察到验证集上的最小$\mathrm{PPL}$结果。验证集上的损失数值约4.8，这是最好的验证上的损失值。而对于Wikitext-103数据集，在训练集上优化参数需要大约5-10个轮数，因为Wikitext-103训练集大约是Wikitext-2的50倍。此外，训练集损失数值大约在5.1左右，因为在 wikitext-103 数据集上更高的原因是我们预测的词汇量要大得多，所以混淆程度（即训练损失）应该更高。虽然我们发现模型可以很容易地收敛到局部最小点，验证误差在这个局部最小值的范围内振荡，但是Wikitext-2和Wikitext-103的唯一区别是训练数据的大小，表示收集很多训练数据不一定能保证模型在相同的验证和测试集上学得更好，训练集大小增大的同时词表也增大了3倍。

此外，我们在实验中发现在 OBW 数据集上运行实验相当具有挑战性，因为它需要更大的参数来拟合。所以我们应用CUDA实现的RNN模型~\upcite{DBLP:journals/corr/AppleyardKB16}，这将会把RNN部分所需要的计算时间降到最低，然而等待模型收敛到训练集最小值仍然需要480小时。

\section{影响因素比较}
这部分将讨论语言模型的大词表问题在具体实验中瓶颈和各种不同优化策略对对该问题的计算效率和性能的提升和分析。
\subsection{词表层次化比较}
首先在wikitext-103数据集，我们统计了语言模型中每个模块的计算时间消耗，如图~\ref{fig:rnn_timing}~所示。 我们计算运行不同的RNN模型（即LSTM模型和GRU模型）及其梯度（即，RNN Grad）所需的时间，以及Wikitext-103数据集的大词汇表上的常规softmax，这词汇表大小与我们平时所采用的数据集的词表相当。 我们分别用Theano框架和CUDA实现了RNN单元，一个采用python语言实现，第二个使用并行C++语言实现。目前基于CuDNN实现的RNN模型计算时间最快，它里面的RNN的运行时间可以缩短到最短。 我们将代码重复了100遍，统计了每个模块的总时间占用，然后分别计算其平均时间占用。这样做的目的是通过多次实验来保证实验数据准确性。
\begin{figure}[!t]
  \centering
  \includegraphics[width=.9\columnwidth]{./figures/rnn_timing.pdf}
  \caption{wikitext-103数据集上测量语言模型三个部分的计算时间比较}\label{fig:rnn_timing}
\end{figure}


从图中可以看出，使用优化的CUDA，softmax模块的影响比RNN单元及其梯度更重要。Softmax计算时间占用总计算时间随着词表的增大占比越来越大，并且已经超过了$50\%$的计算时间，这就需要我们详细讨论softmax优化。

为了对用于训练相同批处理数据的经验时间复杂性和内存消耗进行基准测试，我们使用这些算法在表~\ref{tab:time}~中收集了GPGPU和CPU上的详细结果。我们尝试使用这些算法处理WikiText-103数据集，并计算处理一个批处理数据所需的平均时间。另外，输入句子的最大长度，隐藏层，输出词汇和批量大小分别设置为$\{50, 256, 267735, 20\}$。此外，“总时间”过程表示前向传播和后向梯度优化的过程，“前进时间”过程表示从输入数据到计算模型成本所需的时间消耗。此外，我们计算了上述算法训练期间加载所需的内存。 $\mathcal{|V|} $表示词汇大小，$\mathcal{|H|} $是隐藏层维度。在训练期间，tHSM只消耗最小的存储器资源，而p-tHSM算法覆盖了更大的存储器集合，并且p-tHSM在考虑存储器消耗和速度时采用更大的存储器并且获得了更好的加速比。

\begin{table}[!t]
  \centering
  \caption{Wikitext-103数据集上GPGPU和CPU的运行时内存和计算时间比较\label{tab:time}}
\begin{tabular}{lccccc}
  \toprule
 \multirow{2}{*}{算法}  &\multirow{2}{*}{运行时内存占用} &\multicolumn{2}{c}{总计算时间 (ms)} & \multicolumn{2}{c}{前向计算时间 (ms)}   \\
   \cmidrule(lr){3-4}  \cmidrule(lr){5-6}
	& & CPU&GPGPU & CPU& GPGPU \\ \midrule
Softmax & $\mathcal{|HV|}$ &510.4  &262.1&352.2& 62.9 \\
cHSM    & $2\mathcal{|H|\sqrt{|V|}}$&506.5  &\textbf{40.6}&28.7&14.6 \\
tHSM    &$\mathcal{|H|}$&1,004.0 &444.4 & 8.1&  5.6   \\
p-tHSM  &$\mathcal{|H|\log{|V|}}$ &\textbf{383.5}&	86.4 &\textbf{7.0}&	\textbf{1.4} \\
  \bottomrule
\end{tabular}
\end{table}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.87\columnwidth]{./figures/all_time.pdf}
  \caption{cHSM, tHSM 和 p-tHSM 算法随着词表大小的计算时间影响}\label{fig:hsm_benchmark}
\end{figure}


为了验证与词汇大小相关的cHSM，tHSM和p-tHSM算法的可扩展性，结果展示在图~\ref{fig:hsm_benchmark}~中。 为了显示p-tHSM算法的影响，我们在这里不包含“Softmax”方法，因为与其他算法相比，它消耗了更多的计算时间，无法被放在这个图里面。 很显然，cHSM随着词汇大小的平方根（即，$ \mathcal{O(| H | \sqrt{| V |})} $）进行缩放，而p-tHSM随着词汇大小的增加呈现出稳定的表现。tHSM算法随着此表大小的对数进行变化 （即，$ \mathcal{O(| H | \log{| V |})} $）,我们的p-tHSM算法所需要的计算时间词表变大，与tHSM算法的差异越来越大。这一结果证明我们提出的计算模型更加优越。

在这些实验的基础上，我们可以得出结论：所提出的p-tHSM方法胜过历史记录$ \mathcal{O(| H | \log| V |)})$，并且取得了令人满意的分层softmax方法的加速比。 这种性能归因于基于GPGPU并行性的加速，也是由于p-tHSM 方法的基本结构，能够将目标字树进行并行地计算。


\subsection{搜索策略的影响}
由于我们提出了三个关于推理阶段搜索策略的算法，其影响可以通过WER度量来观察。在这个结构化预测过程中，一个合理的搜索规则将帮助模型获得最小风险的最佳候选人，如表~\ref{tab:search}~所示。 算法~\ref{alog:global}表示我们计算所有单词的得分，单词的概率用整个词汇全局归一化。

对于p-tHSM系列算法，我们比较了所提出的算法~\ref{alog:greed_argmax}和传统的算法~\ref{alog:global}。算法~\ref{alog:greed_argmax}比算法~\ref{alog:global}方法计算结果花费的时间更少。由于基于树的模型在逐层访问路径的过程中更容易出现错误情况，所以算法~\ref{alog:global}的WER分数要比算法~\ref{alog:greed_argmax}~要好。
\begin{table}[!ht]
  \centering
  \caption{Wikitext-2数据集上p-tHSM算法针对不同搜索算法的WER评测结果\label{tab:psearch2}}
\begin{tabular}{llccc}
  \toprule
        & 算法类别&计算时间（ms）&验证集（WER）& 测试集（WER）\\ \midrule
  \multirow{2}{*}{p-tHSM}  &全局计算（算法~\ref{alog:global}）&161& \textbf{76.67\%}&\textbf{75.35\%}\\
        &贪心计算（算法~\ref{alog:greed_argmax}）&\textbf{30} & 79.61\%&79.32\%\\
  \bottomrule
\end{tabular}
\end{table}

值得注意的是，对于cHSM方法算法~\ref{alog:greed_argmax}~比算法~\ref{alog:exact}~得到更好的WER，尽管后一种方法获得了词汇表上的确切顶级候选。由于类结构中存在标签偏差问题，排名最高的词容易出现算法模型无法建模的小组。最后但并非最不重要的是，算法~\ref{alog:exact}~和~\ref{alog:global}获得相同的单词排序，唯一的区别是算法~\ref{alog:exact}~避免了冗余计算。所以他们达到了可比的WER得分，但算法~\ref{alog:exact}~需要更少的时间。

\begin{table}[!ht]
  \centering
  \caption{Wikitext-2数据集上cHSM算法针对不同搜索算法的WER评测结果\label{tab:search}}
\begin{tabular}{llccc}
  \toprule
   & 算法类别&计算时间 (ms)&验证集（WER）& 测试集（WER）\\ \midrule
  \multirow{3}{*}{cHSM} &算法~\ref{algo:alls}&102& 80.00\%& 80.02\%\\
        &算法~\ref{alog:exact}&63& 80.00\%& 80.02\%\\
        &算法~\ref{alog:cargmax}&\textbf{44}&\textbf{ 77.09\%}&\textbf{ 77.07\%}\\
  \bottomrule
\end{tabular}
\end{table}


\subsection{循环网络模型的影响}

从表~\ref{tab:rnn}~中可以看出，拥有门控单元（即LSTM和GRU单元）的RNN模型在PPL和WER指标方面比传统的RNN Relu和RNN Tanh模型表现得更好。因为门控功能可以避免梯度消失的问题，虽然它需要稍微多一些的时间进行计算。此外，LSTM的计算公式在GPGPU上比GRU单元更容易并行运行，因此LSTM比GRU模型需要更少的计算时间，尽管LSTM模型计算量更大。
\begin{table}[!t]
  \centering
  \caption{Wikitext-2数据集上不同循环网络针对 PPL、 WER和计算时间的影响\label{tab:rnn}}
\begin{tabular}{lccc}
  \toprule
  循环神经网络 & 计算时间（ms）&验证集（PPL / WER） & 测试集（PPL / WER）\\ \midrule
  1$\times$RNN Relu~\upcite{DBLP:journals/jmlr/GutmannH10} &176.4&260.52 / 80.00\%&238.75 / 80.02\%\\
  1$\times$RNN Tanh~\upcite{DBLP:journals/iclr/JiVSAD15}   &176.2&250.57 / 79.61\%&230.98 / 79.32\%\\
  1$\times$LSTM~\upcite{7508408}                  &\textbf{189.5}&180.98 / 77.16\%&165.60 / 76.67\%\\
  1$\times$GRU~\upcite{DBLP:journals/corr/ChungGCB14}      &191.3&\textbf{179.59 / 77.09\%}&\textbf{165.32 / 77.07\%}\\ \midrule
  2$\times$RNN Relu~\upcite{DBLP:journals/jmlr/GutmannH10} &266.3&190.52 / 73.01\%&198.75 / 73.02\%\\
  2$\times$RNN Tanh~\upcite{DBLP:journals/iclr/JiVSAD15}   &266.3&189.57 / 72.62\%&260.98 / 72.32\%\\
  2$\times$LSTM~\upcite{7508408}                  &\textbf{279.4}&164.98 / 71.17\%&165.60 / 71.67\%\\
  2$\times$GRU~\upcite{DBLP:journals/corr/ChungGCB14}      &281.2&\textbf{158.59 / 70.08\%}&\textbf{155.32 / 70.07\%}\\
  \bottomrule
\end{tabular}
\end{table}

对于传统的RNN模型，通过时间反向传播（Back Propagation Through Time，BPTT）进行训练，梯度计算步骤中的努力花费在该训练批次中最长序列的长度上是线性的。因此，小批量生产可能效率低下，因为批次中的次序可能有不同的长度。这个问题的一个经典的解决方案是截断~BPTT~算法，因此RNN模型的梯度在截断长度上是恒定的，并且对于GPGPU设备上的小批量处理是高效的。而前向概率计算步骤仍然是相同的，唯一的影响是避免梯度反向传播步骤。从图中可以看出，较小的截断BPTT在时间效率上取得了较好的效果，较长的截断BPTT长度在PPL度量方面效果更好。
\begin{figure}[!ht]
  \centering
  \includegraphics[width=.8\columnwidth]{./figures/minibatch.pdf}
  \caption{BPTT和截断BPTT算法示意图}\label{fig:minibatch}
\end{figure}

在训练过程中，如果某些依赖关系存在在被截断的语句中，则模型可以学习这种关系； 如果这些依赖关系总是跨越不同的语句块，那么截断的语句已经打破了这些依赖，模型将无法学习这种关系。 这是由于模型的参数导数无法回溯到最初的跨段的那个单词，并告诉前段中的某些内容对下一段是有用的。 
如果在一个语句段中存在这种关系的话，模型能学习到这种关系是由于模型的参数能求导到前面对应的单词。
与之相区别的是，在测试时模型将能够在跨语句段中预测， 因为模型的计算步骤是不断向前传播，不存在模型反向求导的计算过程。
\begin{figure}[!ht]
  \centering
  \includegraphics[width=.85\columnwidth]{./figures/tbptt.pdf}
  \caption{Wikitext-2数据集上 BPTT和截断BPTT算法对RNN的影响}\label{fig:tbptt}
\end{figure}
\subsection{采样近似算法比较}

基于抽样的算法的效率和准确性与样本大小密切相关，我们在这小节评测了两种采样算法的性能。我们测试不同采样样本数$k$对模型的训练和测试的结果的影响，结果展示在图~\ref{fig:blackout_nce}中。从图中分析，Blackout算法收敛性比传统的NCE模型相对更好，但是NCE算法计算效率比Balckout更高。所以单位时间里面NCE收敛速度更快，单位参数迭代更新步骤中Balckout算法收敛更快，这与Ji等人的实验结果是一致的~\upcite{DBLP:journals/iclr/JiVSAD15}。同时，我们还发现，当$k=1$时，采样函数计算的就是真正的二元分类概率，此时采样算法振荡现象很剧烈，无法收敛。当逐渐增大采样样本数$k$，采样算法才能逐渐开始收敛。两种算法的加速比是$ V / k $，与$k$呈负相关关系。



\begin{figure}[!ht]
  \centering
  \includegraphics[width=.85\columnwidth]{./figures/nce_blackout.pdf}
  \caption{Wikitext-2上测试不同采样数量对NCE和Blackout算法的影响}\label{fig:blackout_nce}
\end{figure}


\subsection{单词聚类策略分析}
Chen~等人发现cHSM和ptHSM算法的性能对单词的层次聚类算法很敏感~\upcite{DBLP:conf/acl/ChenGA16}。同样，为了获得较为稳定的cHSM算法和p-tHSM方法，我们考虑了几种现有的聚类准则，如表~\ref{table:p-thsm}~和表~\ref{table:clustering}~所示。


在对于基于二叉树的层次概率算法初始化过程中，我们采用了霍夫曼聚类，布朗聚类和词向量聚类方法来生成不同的单词路径分布，详细的结果在表~\ref{table:p-thsm}~中给出。与能调整分支因子的~cHSM~方法不同，p-tHSM算法由于聚类结果最后都合并到根节点，所以没有相关变量可以测量。Uni-gram 聚类方法（即按照词频合并）在创建单词层次结构方面效率更高，而Bi-gram和语义聚类需要花费巨额时间来计算词汇表中单词之间的相似度矩阵。尽管如此，对于单词树合并的规则，Bi-gram 方法考虑了二元共现单词的统计信息，语义聚类方法测度的是特征空间中的欧氏距离。在困惑度量指标衡量下，采用Bi-gram和语义聚类的方法比采用霍夫曼聚类的方法有更好的结果。最后，与~cHSM~方法相比，更深层次的树模型更适合于词汇聚类，适当的聚类可以提高树层次的效率。
\begin{table}[!ht]
  \centering
   \caption{Wikitext-2上评价不同聚类方法对p-tHSM 算法的PPL影响\label{table:p-thsm}}
  \begin{tabular}{lcccc} \toprule
  算法  &建树时间&最大树深度 &验证集 （PPL） & 测试集 （PPL）  \\ \midrule
  Uigram  &3分钟&12 &218.42& 216.05     \\
  Bigram  &35小时&21& 186.23& 189.58\\
  Semantic &26小时 &18& \textbf{163.12} & \textbf{178.78}\\
\bottomrule
  \end{tabular}
\end{table}

\begin{table*}[!ht]
  \centering
  \caption{Wikitext-2数据集上不同聚类算法对cHSM 算法的PPL 和 WER 影响\label{table:clustering}}
  \begin{tabular}{lclccc} \toprule
聚类算法 & 均匀划分？&分支数& 训练轮数& 测试集 （PPL / WER）&耗时 （ms）\\ \midrule
  \multirow{6}{*}{Random}  &\multirow{6}{*}{是}&10/3330&145&211.15 / 78.55 &791\\
    &&20/1664&123&228.72 / 78.89&565\\
    &&40/832&103&234.36 / 79.21&321\\
    &&80/417&78&243.12 / 79.64&171\\
    &&160/208 &57&253.38 / 80.08&92\\
    &&182/183&48&268.63 / 80.11&88\\
  \midrule
  \multirow{6}{*}{Alphabet}  &\multirow{6}{*}{是}&10/3330 &141&199.01 / 78.07 &773\\
    &&20/1664 &120&211.34 / 78.23&551\\
    &&40/832 &100&238.75 / 79.02&313\\
    &&80/417 &90&241.75 / 79.34&174\\
    &&160/208 &56&248.35 / 79.62&97\\
    &&182/183&45&258.57 / 80.02&87\\
  \midrule
  \multirow{6}{*}{Uni-gram}   &\multirow{6}{*}{是} &10/3330&134&211.51 / 77.41 &788\\
    & &20/1664&122&220.01 / 77.71&549\\
    & &40/832&113&236.56 / 77.95&302\\
    & &80/417&91& 241.12 / 78.25&170\\
    & &160/208&55&247.25 / 79.21&93\\
    & &182/183&42&253.35 / 79.92&86\\
  \midrule
  \multirow{5}{*}{Bi-gram}   &\multirow{5}{*}{否}&10/3672&150&208.11 / 77.32&801\\
     &&20/1923&121&217,34 / 77.64&621\\
     &&40/1123&102&228.87 / 78.14&588\\
     &&80/572&89&246.32 / 78.43&186\\
     &&160/340&76&252.33 / 79.51&97\\
  \midrule
  \multirow{5}{*}{Syntactic}  &\multirow{5}{*}{否}&10/3612 &152&214.31 / 78.11&810\\
    &&20/1972 &130&220.19 / 78.86&633\\
    &&40/996 &101&232.33 / 79.33&543\\
    &&80/545 &89&241.34 / 79.84&179\\
    &&160/235 &70&262.34 / 80.14&134\\
  \midrule
  \multirow{5}{*}{Semantic}  &\multirow{5}{*}{否} &10/3570 &133&208.77 / 77.41&819\\
    & &20/1873 &114&218.31 / 77.78&641\\
    & &40/1092 &91&225.38 / 78.35&521\\
    & &80/561 &69&238.45 / 78.91&174\\
    & &160/244 &44&256.75 / 79.41&103\\
\bottomrule
  \end{tabular}
\end{table*}

对于表~\ref{table:clustering}，我们将前面所有提到的聚类方法应用在Wikitext-2数据集上，，同时比较了不同的分支因子对算法效果的影响。从该表的结果分析得出，相比于其他算法随机方法的性能最差，因为它们没有提供有效的先验信息。
此外还观察到，该方法比其他人更难以接受可接受的训练损失，因此在训练集上花费更多的时间来优化参数。然后，发现在对类结构注入外部单字和双字的知识之后，模型确实在目标空间中学习了一个明确定义的单词分布。因此，它对随机和字母表系列取得了更好的结果。而且，用语法和语义算法进行分词的词汇比上述方法得到的结果要好得多，代价是计算分割方法。从实验结果可以看出，向具有先验知识的类结构注入会增强该方法的稳定性，通过平衡聚类时间和模型的准确性，可以调整分支因子，达到预期的结果。



\section{模型总体评价}
如表~\ref{tab:summary_ppl}~所示，我们收集了上述三种标准语料库的验证和测试数据集的所有困惑和错误率结果。值得注意的是，我们采用了一层GRU单元作为所有这些算法的上下文表示，其维数设置为256。另外，对于NCE和Blackout近似，超参数$ k $是 对较小的Wikitext-2和Wikitext-103数据集设置为$\mathcal{|V|}/20 $，对于较大的One Billion Words数据集，设置为$ k=|\mathcal{V}|/200 $。 此外，对于~cHSM~方法，我们根据单词的单字分布来划分词汇。

考虑到Wikitext-2数据集上的结果，最初的softmax比其他算法获得了最好的分数，因为在Blackout和NCE近似中没有引入任何cHSM和p-tHSM算法的结构损失或基于抽样的变分损失。

\begin{table}[!ht]
  \centering
  \caption{所有模型在三个数据集上的PPL和WER的性能评测\label{tab:summary_ppl}}
\begin{tabular}{llcc}
  \toprule
数据集& 算法& 验证集（PPL/WER） & 测试集（PPL/WER） \\ \midrule
 \multirow{2}{*}{WikiText-2}&GRU + Softmax&172.64 / 77.49\%&162.09 / 77.07\% \\
  &GRU + NCE~\upcite{DBLP:journals/jmlr/GutmannH10}&217.84 / 78.26\%&199.54 / 78.02\%\\
  &GRU + Blackout~\upcite{DBLP:journals/iclr/JiVSAD15}&221.15 / 77.72\%&199.56 / 77.50\% \\
  &GRU + cHSM + Uni-gram~\upcite{DBLP:conf/acl/ChenGA16}&253.18 / 78.25\%&236.61 / 78.02\%\\
  &GRU + p-tHSM + Uni-gram~\upcite{DBLP:conf/nips/MikolovSCCD13}&218.42 / 78.15\%&216.05 / 78.15\%\\
  &GRU + p-tHSM + Bi-gram~\upcite{DBLP:journals/coling/BrownPdLM92}&186.23 / 78.15\%&189.58 / 78.15\%\\\midrule
   \multirow{2}{*}{WikiText-103} &GRU + Softmax&130.38 / 72.15\%&136.83 / 72.37\%\\
 &GRU + NCE~\upcite{DBLP:journals/jmlr/GutmannH10}&164.78 / 73.22\%&165.01 / 73.34\%\\
  &GRU + Blackout~\upcite{DBLP:journals/iclr/JiVSAD15}&163.99 / 73.18\%&162.76 / 74.22\%\\
  &GRU + cHSM + Uni-gram~\upcite{DBLP:conf/acl/ChenGA16}&171.81 / 73.42\%&166.74 / 73.18\%\\
  &GRU + p-tHSM + Uni-gram~\upcite{DBLP:conf/nips/MikolovSCCD13}&165.70 / 73.53\%&166.11 / 72.44\%\\
  &GRU + p-tHSM + Bi-gram~\upcite{DBLP:journals/coling/BrownPdLM92}&164.15 / 78.15\%&163.55 / 77.85\%\\\midrule
  \multirow{2}{*}{One Billion Words} &GRU + Softmax&330.38 / 88.15\%&330.83 / 88.37\%\\
 & GRU + NCE~\upcite{DBLP:journals/jmlr/GutmannH10}&272.07 / 84.83\%&276.11 / 84.34\%\\
  &GRU + Blackout~\upcite{DBLP:journals/iclr/JiVSAD15}&268.67 / 84.23\%&266.11 / 84.18\%\\
 & GRU + cHSM + Uni-gram~\upcite{DBLP:conf/acl/ChenGA16}&225.36 / 80.32\%&224.11 / 79.42\%\\
 & GRU + p-tHSM + Uni-gram~\upcite{DBLP:conf/nips/MikolovSCCD13}&231.44 / 87.53\%&236.11 / 82.53\%\\
  %&GRU + p-tHSM + Bi-gram~\upcite{DBLP:journals/coling/BrownPdLM92}& 221.55 / 81.15\%&218.70 / 83.15\%\\
  \bottomrule
\end{tabular}
\end{table}

对于第二个Wikitext-103数据集，布朗聚类的p-tHSM方法不仅获得了比霍夫曼聚类方法更好的结果，而且表现也比其他方法好。另外，cHSM模型能够获得与p-tHSM变体类似的结果，表明我们可以用其他合适的用于cHSM方法的聚类算法获得更好的结果。由于Wikitext-103和Wikitext-2数据集共享相同的测试集，因此发现最初的softmax通过更大的训练数据被收敛到更好的结果。此外，对于采样方法，它收敛于比softmax方法好得多的结果，同时提高了时间效率。

综上所述，在用字极性编码方案替代tHSM中的传统霍夫曼编码方案并且实现了基于紧密的树型模型p-tHSM之后，我们证明了这种新颖的编码方案允许在GPGPU上并行运行计算。这将原始tHSM的时间复杂度从$ \mathcal {O(| H | \log | V |)} $减少到$\mathcal{O(| H || V |)} $并获得了最佳的加速比对于大量的词汇问题。此外，为了稳定p-tHSM模型的性能，我们测试了几种现有的层次聚类算法，发现基于树模型的词聚类与内部节点的二元分类密切相关。

\section{本章小结}

在本章中，首先定量分析了大词表问题，接下来比较了我们提出的模型的计算效率和传统算法之间的差异。针对层次模型的初始化算法，我们讨论并评估三种不同的层次聚类算法，以更有效的方式组织结构化的词表。结果表明，与其他概率归一化方法相比，我们提出的层次概率算法具有很好的加速比，与其他基于抽样的优化算法相比，它性能相对更好。

%在未来的研究中，我们计划探索softplus函数的应用场景，并探讨在cHSM算法训练的时候的词表动态交换算法。另外，实验中应用的几种聚类算法消耗了很多时间，我们可以优化该聚类算法，设计更高效的分层结构。
