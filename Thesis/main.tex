\documentclass[master,openright,twoside,color]{buaathesis}
\usepackage{booktabs}
\usepackage{courier}
\usepackage{minted}
\usemintedstyle{pastie}
\setmonofont{Source Code Pro}
\renewcommand{\theFancyVerbLine}{\sffamily
\textcolor[rgb]{0.5,0.5,1.0}{\scriptsize
\oldstylenums{\arabic{FancyVerbLine}}}}

\usepackage{graphicx}
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{算法}
\SetKwInput{KwData}{输入}
\SetKwInput{KwResult}{输出}
\setmainfont{Times New Roman}
\begin{document}

% 用户信息
\include{data/info}

% 中英封面、提名页、授权书
%\maketitle
% 前言页眉页脚样式
\pagestyle{frontmatter}
% 摘要
\begin{cabstract}
最近在自然语言处理中，已经提出了基于神经网络结构的变体，并成功地应用于神经语言模型和神经声学模型。这些神经模型可以通过学习来自大规模语料库的参数来利用知识，而对于真实世界的挑战来说，它们是非常缓慢的，因为它们在训练和推理期间预测来自大词汇量的候选者。作为基于抽样的近似的替代，我们探索历史提出的基于树和基于类的分层softmax方法。


在本论文研究中，为了减少计算时间，并使其与通用图像计算单元单元（GPGPU）兼容，我们引入了一系列高效和有效的方法，并将我们的贡献归类为：a）首先，我们改进它们的结构组成并引入一个紧凑的基于树和基于类的相应的分层softmax方法的损失函数; b）其次，我们讨论几种基于ngram的句法和语义聚类算法对各种类型层级softmax的影响，因为它们对聚类方法敏感; c）第三，我们讨论了可能的分层softmax变体的推理算法，保证模型可以在不同情况下获取可能的预测结果。

最后，我们用标准基准数据集，即WikiText-2，WikiText-103和十亿字数据集对语言建模任务进行了我们的实验。与其他传统的优化方法相比，还实现了几个内在的评估指标的提升：单词错误率和困惑度。
\end{cabstract}

\begin{eabstract}
Recently in natural language processing, variants of neural networks based architecture have been proposed, and successfully applied to neural language models and neural acoustic models. These neural models can leverage knowledge by learning parameters from massive corpora, while they are extremely slow for the real world challenge as they predict candidates from a large vocabulary during training and inference. As an alternative to sampling-based approximation, we explore the historical proposed tree-based and class-based hierarchical softmax methods.

In this research, aiming at reducing its computational time as well as making them compatible with modern graphics processing units, we introduce a series of efficient and effective approaches and categorise our contributions as: a) Firstly, we reform their structural composition and introduce a compact tree-based and class-based loss function for the corresponding hierarchical softmax methods; b) Secondly, we discuss the impact of several ngram-based, syntactic and semantic clustering algorithms for the vanilla hierarchical softmax as they are sensitive to clustering methods; c) Thirdly, we discuss possible inference algorithms for the hierarchical softmax variants, assuring the model can fetch presumable predictions under different circumstances.

Finally, Our experiments were carried out on language modelling tasks with standard benchmarks datasets, i.e., WikiText-2, WikiText-103 and One Billion Word datasets. Consist improvement with several intrinsic evaluation criterions: word error rate and perplexity, were also achieved over other conventional optimisation methods.
\end{eabstract}
% 目录、插图目录、表格目录
\tableofcontents
\listoffigures
\listoftables
% 符号表
%\include{data/master/denotation}

% 正文页码样式
\mainmatter
% 正文页眉页脚样式
\pagestyle{mainmatter}

\include{data/chapter-1-introduction}
\include{data/chapter-2-relatedwork}
\include{data/chapter-3-models}
\include{data/chapter-4-experiment}
\include{data/chapter-5-summary}

% 参考文献
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{参考文献}
\nocite{*}
\bibliographystyle{GBT7714-2005}
\bibliography{thesis_reference}
\cleardoublepage

% 附录
%\appendix

% 附页标题样式
\backmatter
%\include{data/chapter-6-appendix}

\end{document}
